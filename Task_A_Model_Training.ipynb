{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tpot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwCMpYwJ2rsi",
        "outputId": "f1e78832-ff22-41e1-ea05-d7d678e2900d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tpot\n",
            "  Downloading TPOT-0.12.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.5.2)\n",
            "Collecting deap>=1.2 (from tpot)\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting update-checker>=0.16 (from tpot)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (4.66.6)\n",
            "Collecting stopit>=1.1.1 (from tpot)\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.2.2)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.4.2)\n",
            "Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.1->tpot) (3.5.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.16->tpot) (2.32.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost>=1.1.0->tpot) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->tpot) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2024.8.30)\n",
            "Downloading TPOT-0.12.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Building wheels for collected packages: stopit\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11939 sha256=c998adc548e4a6553c7cfdec1fae873638ea4d9b0a8dd4c5310ce6c912b2c9b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/f9/87/bf5b3d565c2a007b4dae9d8142dccc85a9f164e517062dd519\n",
            "Successfully built stopit\n",
            "Installing collected packages: stopit, deap, update-checker, tpot\n",
            "Successfully installed deap-1.4.1 stopit-1.1.2 tpot-0.12.2 update-checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('Final_Training.csv')  # Adjust path\n",
        "val_data = pd.read_csv('Final_val.csv')  # Adjust path\n",
        "\n",
        "# Extract features and labels\n",
        "X_train = train_data['Cleaned_Tweet']\n",
        "y_train = train_data['Hate']  # You can choose 'Fake' as well\n",
        "\n",
        "X_val = val_data['Cleaned_Tweet']\n",
        "y_val = val_data['Hate']\n",
        "\n",
        "# Convert text data into numerical format using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust number of features if needed\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Convert sparse matrices to dense format\n",
        "X_train_dense = X_train_tfidf.toarray()\n",
        "X_val_dense = X_val_tfidf.toarray()\n",
        "\n",
        "# Train-test split (if needed, you can skip this if already splitting correctly)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_dense, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize TPOT\n",
        "tpot = TPOTClassifier(verbosity=2, generations=5, population_size=20, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tpot.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = tpot.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "de00dbc420c848e9bd787609e68893fb",
            "29238fec5cbf4c3cb914bc7ab048ff9d",
            "ef12fc4e7935413f97426a5510fcdabb",
            "8bb822343c404fa496c5226b6ba57625",
            "9248d8c83d6c4796871a2c94e0f8c3a8",
            "29dd12503ecc41ce9cbe56c46b22fba4",
            "36d7752bc29c4ece9eb0fb8a4544fe94",
            "f41c7800eee4410abda6ce32fa611cb0",
            "3ec990c886b8411aadba5a9e6c488462",
            "6fdb275c12764f2486b22e453a50af86",
            "82b8d7cf21c14ceb8187c4276c68f0d2"
          ]
        },
        "id": "VdoLExR03JoF",
        "outputId": "62d53457-23ce-48a3-dc4d-2b97e4275d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de00dbc420c848e9bd787609e68893fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generation 1 - Current best internal CV score: 0.8465417000179943\n",
            "\n",
            "Generation 2 - Current best internal CV score: 0.8489048975261071\n",
            "\n",
            "Generation 3 - Current best internal CV score: 0.8489048975261071\n",
            "\n",
            "Generation 4 - Current best internal CV score: 0.8489048975261071\n",
            "\n",
            "Generation 5 - Current best internal CV score: 0.8489048975261071\n",
            "\n",
            "Best pipeline: LogisticRegression(MultinomialNB(input_matrix, alpha=0.1, fit_prior=False), C=10.0, dual=False, penalty=l2)\n",
            "Accuracy: 0.8714555765595463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwIrWWszalb8",
        "outputId": "ae5d6a18-c701-4111-e2a4-2d005667565a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('Final_Training.csv')\n",
        "val_data = pd.read_csv('Final_val.csv')\n",
        "\n",
        "# Combine train and validation data for robust training\n",
        "data = pd.concat([train_data, val_data], ignore_index=True)\n",
        "\n",
        "# Ensure no missing values\n",
        "data = data.dropna(subset=['Cleaned_Tweet', 'Hate', 'Fake'])  # Ensure all required columns are present\n",
        "\n",
        "### Helper Function to Train and Save Model ###\n",
        "def train_and_save_model(task_name, target_column):\n",
        "    print(f\"\\nTraining model for: {task_name}\")\n",
        "\n",
        "    # Extract features and labels\n",
        "    X = data['Cleaned_Tweet']\n",
        "    y = data[target_column]\n",
        "\n",
        "    # Check class imbalance and compute class weights\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class Weights for {task_name}:\", class_weight_dict)\n",
        "\n",
        "    # Split data into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "    # Define the pipeline with TF-IDF and Logistic Regression\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=7000)),\n",
        "        ('lr', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    # Evaluate with StratifiedKFold\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "    print(f\"Stratified K-Fold Cross-Validation Accuracy for {task_name}: {cv_scores.mean():.4f}\")\n",
        "\n",
        "    # Train the pipeline on the training data\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the training and validation sets\n",
        "    y_train_pred = pipeline.predict(X_train)\n",
        "    y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "    # Training Evaluation Metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    train_precision = precision_score(y_train, y_train_pred, average='binary')\n",
        "    train_recall = recall_score(y_train, y_train_pred, average='binary')\n",
        "    train_f1 = f1_score(y_train, y_train_pred, average='binary')\n",
        "    train_macro_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "    print(f\"\\n{task_name} - Training Metrics:\")\n",
        "    print(f\"Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}, Macro F1: {train_macro_f1:.4f}\")\n",
        "\n",
        "    # Validation Evaluation Metrics\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    val_precision = precision_score(y_val, y_val_pred, average='binary')\n",
        "    val_recall = recall_score(y_val, y_val_pred, average='binary')\n",
        "    val_f1 = f1_score(y_val, y_val_pred, average='binary')\n",
        "    val_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
        "    print(f\"\\n{task_name} - Validation Metrics:\")\n",
        "    print(f\"Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
        "\n",
        "    # Print classification report for validation set\n",
        "    print(f\"\\n{task_name} - Classification Report (Validation Set):\")\n",
        "    print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "    # Save the trained pipeline\n",
        "    model_filename = f\"{task_name.lower()}_pipeline_model.pkl\"\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        pickle.dump(pipeline, f)\n",
        "    print(f\"{task_name} pipeline model saved successfully as {model_filename}\")\n",
        "\n",
        "# Train separate models for Hate and Fake\n",
        "train_and_save_model(\"Hate\", \"Hate\")\n",
        "train_and_save_model(\"Fake\", \"Fake\")\n"
      ],
      "metadata": {
        "id": "737bAAD6I5DC",
        "outputId": "682879d5-e838-42ee-aad2-056167e98dec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model for: Hate\n",
            "Class Weights for Hate: {0: 0.9983277591973244, 1: 1.0016778523489933}\n",
            "Stratified K-Fold Cross-Validation Accuracy for Hate: 0.8268\n",
            "\n",
            "Hate - Training Metrics:\n",
            "Accuracy: 0.9028, Precision: 0.8997, Recall: 0.9062, F1: 0.9030, Macro F1: 0.9028\n",
            "\n",
            "Hate - Validation Metrics:\n",
            "Accuracy: 0.8463, Precision: 0.8432, Recall: 0.8503, F1: 0.8468, Macro F1: 0.8463\n",
            "\n",
            "Hate - Classification Report (Validation Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       837\n",
            "           1       0.84      0.85      0.85       835\n",
            "\n",
            "    accuracy                           0.85      1672\n",
            "   macro avg       0.85      0.85      0.85      1672\n",
            "weighted avg       0.85      0.85      0.85      1672\n",
            "\n",
            "Hate pipeline model saved successfully as hate_pipeline_model.pkl\n",
            "\n",
            "Training model for: Fake\n",
            "Class Weights for Fake: {0: 0.9239442847667477, 1: 1.0897001303780964}\n",
            "Stratified K-Fold Cross-Validation Accuracy for Fake: 0.8030\n",
            "\n",
            "Fake - Training Metrics:\n",
            "Accuracy: 0.8774, Precision: 0.8683, Recall: 0.8638, F1: 0.8660, Macro F1: 0.8765\n",
            "\n",
            "Fake - Validation Metrics:\n",
            "Accuracy: 0.8266, Precision: 0.8159, Recall: 0.8031, F1: 0.8095, Macro F1: 0.8251\n",
            "\n",
            "Fake - Classification Report (Validation Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       905\n",
            "           1       0.82      0.80      0.81       767\n",
            "\n",
            "    accuracy                           0.83      1672\n",
            "   macro avg       0.83      0.82      0.83      1672\n",
            "weighted avg       0.83      0.83      0.83      1672\n",
            "\n",
            "Fake pipeline model saved successfully as fake_pipeline_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRC7Py97zUlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_excel('Test_Task_A.xlsx')  # Ensure 'test_data.csv' contains columns 'ID' and 'Cleaned_Tweet'\n",
        "\n",
        "# Ensure no missing values in the required columns\n",
        "test_data = test_data.dropna(subset=['Tweet'])\n",
        "\n",
        "# Load the saved models\n",
        "with open('hate_pipeline_model.pkl', 'rb') as f:\n",
        "    hate_model = pickle.load(f)\n",
        "\n",
        "with open('fake_pipeline_model.pkl', 'rb') as f:\n",
        "    fake_model = pickle.load(f)\n",
        "\n",
        "# Predict Hate labels (0/1)\n",
        "test_data['Hate (0/1)'] = hate_model.predict(test_data['Tweet'])\n",
        "\n",
        "# Predict Fake labels (0/1)\n",
        "test_data['Fake (0/1)'] = fake_model.predict(test_data['Tweet'])\n",
        "\n",
        "# Save the updated test data with predictions\n",
        "output_filename = 'test_data_with_predictions.csv'\n",
        "test_data.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Predictions added and saved to {output_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSwR07tgnfvj",
        "outputId": "aa35268d-7bb3-4cf3-c4c0-6cd567545c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions added and saved to test_data_with_predictions.csv\n"
          ]
        }
      ]
    }
  ]
}